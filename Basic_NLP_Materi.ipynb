{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengenalan Natural Language Processing (NLP)\n",
    "\n",
    "Oleh:\\\n",
    "__Ibnu Pujiono__ \\\n",
    "__Sigit Sumarsono__ \\\n",
    "(_Ministry of Finance Data Analytics Community_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah mengikuti pelatihan ini diharapkan peserta dapat:\n",
    "\n",
    "__üìù Standar Kompetensi__\n",
    "\n",
    "Menjelaskan penggunaan NLP dalam membangun model data analytics di Python\n",
    "\n",
    "__üìù Kompetensi Dasar__\n",
    "\n",
    "1. Menjelaskan konsep Dasar NLP\n",
    "2. Menjelaskan task-task dasar yang dilakukan dalam pengolahan teks\n",
    "3. Menjelaskan proses feature extraction dari suatu teks menggunakan NLP\n",
    "4. Menjelaskan penggunaan NLP dalam pemodelan data analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konsep dasar NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Natural Language__ : Bahasa manusia (misal bahasa Indonesia, Inggris, dsb) bukan bahasa pemrograman.\n",
    "\n",
    "__Processing__ : Pemrosesan oleh komputer.\n",
    "\n",
    "\\\n",
    "__In a Nutshell__: Bagaimana komputer memproses bahasa manusia.\n",
    "\n",
    "\\\n",
    "__Mengapa kita melakukan NLP?__\n",
    "- Teks sebagai \"unstructured data\" yang perlu digali kandungan informasinya.\n",
    "- Perkembangan kekuatan processing dan peningkatan kecepatan access memory semakin memudahkan pemrosesan teks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dimana posisi NLP?__\n",
    "<details>\n",
    "    <img src=\"img/image.png\" alt=\"Posisi NLP dalam Artificial Intelligence\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penerapan NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <img src=\"img/image-1.jpeg\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <img src=\"img/image-2.jpeg\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sumber : Vajjala et al, Practical Natural Language Processing. O'reilly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:47:33.012362Z",
     "start_time": "2021-10-29T01:47:23.358960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sigit\\miniconda3\\envs\\ds\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sigit\\miniconda3\\envs\\ds\\lib\\site-packages (from nltk) (4.62.2)\n",
      "Requirement already satisfied: click in c:\\users\\sigit\\miniconda3\\envs\\ds\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\sigit\\miniconda3\\envs\\ds\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\sigit\\miniconda3\\envs\\ds\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sigit\\miniconda3\\envs\\ds\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Requirement already satisfied: sastrawi in c:\\users\\sigit\\miniconda3\\envs\\ds\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install sastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pengenalan String pada Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dasar-dasar string operation.\n",
    "\n",
    "Sebelum masuk lebih jauh ke NLP, mari kita pelajari dulu pemrosesan string.\n",
    "\n",
    "Mengapa? karena string/text adalah bentuk data NLP yang paling sederhana (dibanding audio, atau visual)\n",
    "\n",
    "Data tekstual dalam Python is diolah dengan _object class_ ```str```, atau strings. \n",
    "\n",
    "__String :__ Immutable sequences of Unicode code points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:52:41.482810Z",
     "start_time": "2021-10-29T01:52:41.475861Z"
    }
   },
   "outputs": [],
   "source": [
    "####################### LAB #########################\n",
    "\n",
    "## Membuat variabel baru untuk menyimpan string\n",
    "nama = 'SiGiT SuMaRsOnO'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:57:03.523605Z",
     "start_time": "2021-10-29T01:57:03.515815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiGiT SuMaRsOnO\n"
     ]
    }
   ],
   "source": [
    "## Memanggil nama dengan perintah Print\n",
    "print(nama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:53:30.850002Z",
     "start_time": "2021-10-29T01:53:30.827153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Mengecek tipe data\n",
    "\n",
    "type(nama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:55:43.274084Z",
     "start_time": "2021-10-29T01:55:43.267872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"SIGIT\" == \"sigit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:57:52.714253Z",
     "start_time": "2021-10-29T01:57:52.694922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sigit Sumarsono'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lowercase, Uppercase, capitalize, etc.\n",
    "\n",
    "#lowercase\n",
    "#nama.lower()\n",
    "\n",
    "#uppercase\n",
    "#nama.upper()\n",
    "\n",
    "#capitalize - Warning, hanya membuat huruf besar pada awal kalimat\n",
    "#nama.capitalize()\n",
    "\n",
    "#title case - membuat huruf besar pada awal kata - padanan capitalize each word\n",
    "#nama.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:02:16.593306Z",
     "start_time": "2021-10-29T02:02:16.578755Z"
    }
   },
   "outputs": [],
   "source": [
    "## Spliting text\n",
    "ttl = \"Sragen, 4 Agustus 1991\"\n",
    "\n",
    "## split variabel (default pada spasi)\n",
    "#ttl.split()\n",
    "\n",
    "## split pada koma\n",
    "#ttl.split(\",\")\n",
    "\n",
    "## Membatasi jumlah split\n",
    "#ttl.split(maxsplit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:59:51.963793Z",
     "start_time": "2021-10-29T01:59:51.948665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sragen, 4 Agustus 1991'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:02:48.113558Z",
     "start_time": "2021-10-29T02:02:48.105494Z"
    }
   },
   "outputs": [],
   "source": [
    "## splitting text menghasilkan list, setelahnya dapat dilakukan operasi list pada umumnya\n",
    "\n",
    "ttl_list=ttl.split()\n",
    "\n",
    "#tgl_lahir=ttl_list[1]\n",
    "#tgl_lahir\n",
    "\n",
    "#ttl_list.insert(1, 'Minggu')\n",
    "#ttl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:04:11.188047Z",
     "start_time": "2021-10-29T02:04:11.163531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barat,'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttl2 = 'Jakarta Barat, 2 Mei 1991'.split()\n",
    "ttl2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:05:20.432367Z",
     "start_time": "2021-10-29T02:05:20.427266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sragen, 4 Agustus 1991'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Join \n",
    "# digunakan untuk menyambung list menjadi teks\n",
    "\n",
    "ttl = \" \".join(ttl_list) # perhatikan penggunaan spasi\n",
    "ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File operation\n",
    "\n",
    "string operation erat kaitannya dengan import plaintext kedalam lingkungan pengolahan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:11:03.456518Z",
     "start_time": "2021-10-29T02:11:03.442874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROKLAMASI\\nKami bangsa Indonesia dengan ini menjatakan kemerdekaan Indonesia.\\nHal-hal jang mengenai pemindahan kekoeasaan d.l.l., diselenggarakan\\ndengan tjara saksama dan dalam tempo jang sesingkat-singkatnja.\\nDjakarta, hari 17 boelan 8 tahoen 05\\nAtas nama bangsa Indonesia.\\nSoekarno/Hatta.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Membuka dokumen teks\n",
    "with open('data/proklamasi.txt', 'r') as f:\n",
    "  teks = f.read()\n",
    "\n",
    "teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:13:13.230893Z",
     "start_time": "2021-10-29T02:13:13.220699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROKLAMASI\n",
      "Kami bangsa Indonesia dengan ini menjatakan kemerdekaan Indonesia.\n",
      "Hal-hal jang mengenai pemindahan kekoeasaan d.l.l., diselenggarakan\n",
      "dengan tjara saksama dan dalam tempo jang sesingkat-singkatnja.\n",
      "Djakarta, hari 17 boelan 8 tahoen 05\n",
      "Atas nama bangsa Indonesia.\n",
      "Soekarno/Hatta.\n"
     ]
    }
   ],
   "source": [
    "## F String\n",
    "# digunakan untuk menerjemahkan escape character (spasi, baris baru, tab)\n",
    "# berlaku untuk python 3\n",
    "# dokumentasi lebih lengkap: http://cis.bentley.edu/sandbox/wp-content/uploads/Documentation-on-f-strings.pdf\n",
    "\n",
    "print(f'{teks}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression (Regex)\n",
    "\n",
    "Merupakan serangakaian karakter untuk mendefinisikan pola pencarian teks. \\\n",
    "Python menyediakan library ```re``` untuk mendukung regex. \\\n",
    "pandas juga mendukung regex (akan dijelaskan nanti).\n",
    "\n",
    "Dokumentasi lebih lanjut: [https://regexr.com/](https://regexr.com/)\n",
    "\n",
    "__Kelas Karakter__\n",
    "\n",
    "|Simbol(Perl/Tcl)|Ekuivalen(ASCII)|Deskripsi|\n",
    "|:-:|:-:|:--|\n",
    "|\\n||Baris baru|\n",
    "|\\t||Tab horizontal|\n",
    "|\\w|[A-Za-z0-9_]|Karakter Alpanumerik (termasuk underscore)|\n",
    "|\\W|[^A-Za-z0-9_]|Karakter non-alpanumerik (termasuk simbol)|\n",
    "|\\d|[0-9]|Digit|\n",
    "|\\D|[^0-9]|Non-digit|\n",
    "|\\s|[ \\t\\r\\n\\v\\f]|Whitespace|\n",
    "|\\S|[^ \\t\\r\\n\\v\\f]|Non-whitespace|\n",
    "\n",
    "\n",
    "__Kelas Kontrol__\n",
    "\n",
    "|Simbol|Deskripsi|\n",
    "|:-:|:--|\n",
    "|\\[123\\]|angka 1, 2, atau 3|\n",
    "|\\[abc\\]|huruf a,b, atau c|\n",
    "|\\[^xyz\\]|selain huruf x, y, atau z|\n",
    "|\\\\.|titik|\n",
    "|\\[a-z\\]|a sampai z|\n",
    "|\\[A-Z\\]|A sampai Z|\n",
    "|\\[0-9\\]|seluruh digit|\n",
    "|n{2}|karakter n sebanyak 2|\n",
    "|n{1,3}| 1 sampai 3 karakter n|\n",
    "|n+|1 atau lebih kemunculan n|\n",
    "|n*|0 atau lebih kemunculan n|\n",
    "|n?|0 atau 1 kali kemunculan n|\n",
    "|\\||Boolean 'atau'|\n",
    "|()|grouping|\n",
    "|.|Wildcard (karakter apapun)|\n",
    "|^|Permulaan kalimat/baris|\n",
    "|$|Akhir kalimat/baris|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:23:20.869061Z",
     "start_time": "2021-10-29T02:23:20.859680Z"
    }
   },
   "outputs": [],
   "source": [
    "## jangan lupa import library re\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:31:21.601782Z",
     "start_time": "2021-10-29T02:31:21.593747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kami bangsa Indonesia dengan ini menjatakan kemerdekaan Indonesia.\n",
      "\n",
      "Atas nama bangsa Indonesia.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Menggunakan search\n",
    "\n",
    "with open('data/proklamasi.txt', 'r') as f:\n",
    "  text = f.readlines() # menyimpan sebagai list teks per baris\n",
    "\n",
    "for line in text:\n",
    "  if re.search('Indonesia', line):\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:24:25.726686Z",
     "start_time": "2021-10-29T02:24:25.716301Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@binofficial_ri', '@BNI']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Menggunakan findall\n",
    "\n",
    "mention = lambda x: re.findall('@\\w+', x) # membuat lambda untuk menemukan username twitter\n",
    "\n",
    "twit = \".@binofficial_ri Halo @BNI ada kembaran\"\n",
    "\n",
    "mention(twit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:26:17.223286Z",
     "start_time": "2021-10-29T02:26:17.205921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROKLAMASI\n",
      "Kami bangsa Indonesia dengan ini menjatakan kemerdekaan Indonesia.\n",
      "Hal-hal jang mengenai pemindahan kekoeasaan d.l.l., diselenggarakan\n",
      "dengan tjara saksama dan dalam tempo jang sesingkat-singkatnja.\n",
      "Djakarta, hari 17 boelan 8 tahoen 05\n",
      "Atas nama bangsa Indonesia.\n",
      "Soekarno/Hatta.\n"
     ]
    }
   ],
   "source": [
    "print(f'{teks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:33:54.500888Z",
     "start_time": "2021-10-29T02:33:54.490169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROKLAMASI\n",
      "Kami bangsa Indonesia dengan ini menjatakan kemerdekaan Indonesia \n",
      "Hal hal jang mengenai pemindahan kekoeasaan d l l   diselenggarakan\n",
      "dengan tjara saksama dan dalam tempo jang sesingkat singkatnja \n",
      "Djakarta  hari 17 boelan 8 tahoen 05\n",
      "Atas nama bangsa Indonesia \n",
      "Soekarno Hatta \n"
     ]
    }
   ],
   "source": [
    "## Menggunakan sub\n",
    "\n",
    "def ubah_ejaan(string):\n",
    "  # mengganti huruf ejaan lama:\n",
    "  string = re.sub(r'(?<=[^DdTt])J', 'Y', string)\n",
    "  string = re.sub(r'(?<=[^DdTt])j', 'y', string)\n",
    "  string = re.sub(r'D[Jj]', 'J', string)\n",
    "  string = re.sub(r'dj', 'j', string)\n",
    "  string = re.sub(r'T[Jj]', 'C', string)\n",
    "  string = re.sub(r'tj', 'c', string)\n",
    "  string = re.sub('O[Ee]', 'U', string)\n",
    "  string = re.sub('oe', 'u', string)\n",
    "  return string\n",
    "\n",
    "def hapus_simbol(string):\n",
    "  return re.sub(r'[^\\n\\w ]', ' ', string)\n",
    "\n",
    "#print(f'{hapus_simbol(ubah_ejaan(teks))}')\n",
    "print(f'{hapus_simbol(teks)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <img src=\"img/image-pre.jpeg\">\n",
    "</details>\n",
    "\n",
    "Alur Analisis Data Ticketing System Uber\n",
    "\n",
    "Sumber: Vajjala et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:43:07.630457Z",
     "start_time": "2021-10-29T02:43:05.024780Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sigit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sigit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Library yang diperlukan\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inisiasi Model\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "porter_stemmer=PorterStemmer()\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_id = list(stopwords.words('indonesian'))\n",
    "stop_en = list(stopwords.words('english'))\n",
    "\n",
    "# Data yang dibutuhkan \n",
    "jokowi_tweet = \"Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing. \\\n",
    "  Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.\"\n",
    "\n",
    "trump_tweet = \"The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future. \\\n",
    "  They will not be disrespected or treated unfairly in any way, shape or form!!!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:43:46.045407Z",
     "start_time": "2021-10-29T02:43:46.019912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jokowi</td>\n",
       "      <td>Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump</td>\n",
       "      <td>The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  \\\n",
       "0  Jokowi   \n",
       "1   Trump   \n",
       "\n",
       "                                                                                                                                                                                                                                                            tweet  \n",
       "0  Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.  \n",
       "1                            The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  'name':['Jokowi', 'Trump'],\n",
    "  'tweet':[jokowi_tweet, trump_tweet]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "Dalam sebagian besar masalah, perbedaan antara huruf besar dan kecil tidak menjadi masalah (kecuali untuk Trump :3). Untuk itu agar mengurangi fitur yang harus diolah, seluruh teks dijadikan huruf kecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:46:04.658875Z",
     "start_time": "2021-10-29T02:46:04.642876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lowercasing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jokowi</td>\n",
       "      <td>Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump</td>\n",
       "      <td>The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  \\\n",
       "0  Jokowi   \n",
       "1   Trump   \n",
       "\n",
       "                                                                                                                                                                                                                                                            tweet  \\\n",
       "0  Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                      lowercasing  \n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.  \n",
       "1                            the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Lowercasing\n",
    "\n",
    "df = df.assign(\n",
    "  lowercasing=df.tweet.apply(\n",
    "    lambda x: x.lower()\n",
    "  )\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal or Punctuation and other Noise\n",
    "\n",
    "Sama seperti lowercasing. dalam banyak masalah, tanda baca dan simbol lain tidak memberi nilai tambah dalam analisis. Karena itu dapat dihapus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:47:17.443549Z",
     "start_time": "2021-10-29T02:47:17.428273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lowercasing</th>\n",
       "      <th>punctremoval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jokowi</td>\n",
       "      <td>Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid19 sekecil apapun di daerahnya masingmasing   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump</td>\n",
       "      <td>The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75000000 great american patriots who voted for me america first and make america great again will have a giant voice long into the future   they will not be disrespected or treated unfairly in any way shape or form</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  \\\n",
       "0  Jokowi   \n",
       "1   Trump   \n",
       "\n",
       "                                                                                                                                                                                                                                                            tweet  \\\n",
       "0  Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                      lowercasing  \\\n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                 punctremoval  \n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid19 sekecil apapun di daerahnya masingmasing   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi  \n",
       "1                                  the 75000000 great american patriots who voted for me america first and make america great again will have a giant voice long into the future   they will not be disrespected or treated unfairly in any way shape or form  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Menghilangkan tanda baca\n",
    "df = df.assign(\n",
    "  punctremoval=(\n",
    "    df.lowercasing.replace(to_replace='[^\\w ]+', value='', regex=True)\n",
    "  ) \n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Memecah suatu kalimat panjang menjadi komponen-komponen kecil yang mudah dianalisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:47:49.963008Z",
     "start_time": "2021-10-29T02:47:49.950456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lowercasing</th>\n",
       "      <th>punctremoval</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jokowi</td>\n",
       "      <td>Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid19 sekecil apapun di daerahnya masingmasing   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi</td>\n",
       "      <td>[saya, meminta, para, kepala, daerah, mewaspadai, kenaikan, kasus, covid19, sekecil, apapun, di, daerahnya, masingmasing, saya, juga, mengingatkan, seluruh, kepala, daerah, agar, terus, mempercepat, vaksinasi, untuk, melindungi, rakyat, sekaligus, mendorong, pertumbuhan, ekonomi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump</td>\n",
       "      <td>The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75000000 great american patriots who voted for me america first and make america great again will have a giant voice long into the future   they will not be disrespected or treated unfairly in any way shape or form</td>\n",
       "      <td>[the, 75000000, great, american, patriots, who, voted, for, me, america, first, and, make, america, great, again, will, have, a, giant, voice, long, into, the, future, they, will, not, be, disrespected, or, treated, unfairly, in, any, way, shape, or, form]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  \\\n",
       "0  Jokowi   \n",
       "1   Trump   \n",
       "\n",
       "                                                                                                                                                                                                                                                            tweet  \\\n",
       "0  Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                      lowercasing  \\\n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                 punctremoval  \\\n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid19 sekecil apapun di daerahnya masingmasing   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi   \n",
       "1                                  the 75000000 great american patriots who voted for me america first and make america great again will have a giant voice long into the future   they will not be disrespected or treated unfairly in any way shape or form   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                      token  \n",
       "0  [saya, meminta, para, kepala, daerah, mewaspadai, kenaikan, kasus, covid19, sekecil, apapun, di, daerahnya, masingmasing, saya, juga, mengingatkan, seluruh, kepala, daerah, agar, terus, mempercepat, vaksinasi, untuk, melindungi, rakyat, sekaligus, mendorong, pertumbuhan, ekonomi]  \n",
       "1                          [the, 75000000, great, american, patriots, who, voted, for, me, america, first, and, make, america, great, again, will, have, a, giant, voice, long, into, the, future, they, will, not, be, disrespected, or, treated, unfairly, in, any, way, shape, or, form]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.assign(\n",
    "  token=df.punctremoval.apply(lambda x: x.split())\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and/or Lemmatization\n",
    "\n",
    "Stemming : menghilangkan imbuhan dan mencari kata dasar suatu kata tanpa mengacu ke kamus. \\\n",
    "Lemmatization : Mencari bentuk dasar suatu kata dengan memetakan kedalam suatu kamus. \n",
    "\n",
    "Secara Komputasi : Stemming > Lemmatization \\\n",
    "Secara Kualitas hasil : Stemming < Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:48:58.501194Z",
     "start_time": "2021-10-29T02:48:56.944042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " '75000000',\n",
       " 'great',\n",
       " 'american',\n",
       " 'patriots',\n",
       " 'who',\n",
       " 'vote',\n",
       " 'for',\n",
       " 'me',\n",
       " 'america',\n",
       " 'first',\n",
       " 'and',\n",
       " 'make',\n",
       " 'america',\n",
       " 'great',\n",
       " 'again',\n",
       " 'will',\n",
       " 'have',\n",
       " 'a',\n",
       " 'giant',\n",
       " 'voice',\n",
       " 'long',\n",
       " 'into',\n",
       " 'the',\n",
       " 'future',\n",
       " 'they',\n",
       " 'will',\n",
       " 'not',\n",
       " 'be',\n",
       " 'disrespect',\n",
       " 'or',\n",
       " 'treat',\n",
       " 'unfairly',\n",
       " 'in',\n",
       " 'any',\n",
       " 'way',\n",
       " 'shape',\n",
       " 'or',\n",
       " 'form']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh Lemmatization pada twit trump, (lemmatization bahasa indonesia belum didukung NLTK)\n",
    "trump_lemme = [lemmatizer.lemmatize(word=word,pos='v') for word in df.token[1]]\n",
    "trump_lemme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:49:10.972379Z",
     "start_time": "2021-10-29T02:49:10.338607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lowercasing</th>\n",
       "      <th>punctremoval</th>\n",
       "      <th>token</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jokowi</td>\n",
       "      <td>Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid19 sekecil apapun di daerahnya masingmasing   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi</td>\n",
       "      <td>[saya, meminta, para, kepala, daerah, mewaspadai, kenaikan, kasus, covid19, sekecil, apapun, di, daerahnya, masingmasing, saya, juga, mengingatkan, seluruh, kepala, daerah, agar, terus, mempercepat, vaksinasi, untuk, melindungi, rakyat, sekaligus, mendorong, pertumbuhan, ekonomi]</td>\n",
       "      <td>[saya, minta, para, kepala, daerah, waspada, naik, kasus, covid19, kecil, apa, di, daerah, masingmasing, saya, juga, ingat, seluruh, kepala, daerah, agar, terus, cepat, vaksinasi, untuk, lindung, rakyat, sekaligus, dorong, tumbuh, ekonomi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump</td>\n",
       "      <td>The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75000000 great american patriots who voted for me america first and make america great again will have a giant voice long into the future   they will not be disrespected or treated unfairly in any way shape or form</td>\n",
       "      <td>[the, 75000000, great, american, patriots, who, voted, for, me, america, first, and, make, america, great, again, will, have, a, giant, voice, long, into, the, future, they, will, not, be, disrespected, or, treated, unfairly, in, any, way, shape, or, form]</td>\n",
       "      <td>[the, 75000000, great, american, patriot, who, vote, for, me, america, first, and, make, america, great, again, will, have, a, giant, voic, long, into, the, futur, they, will, not, be, disrespect, or, treat, unfairli, in, ani, way, shape, or, form]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  \\\n",
       "0  Jokowi   \n",
       "1   Trump   \n",
       "\n",
       "                                                                                                                                                                                                                                                            tweet  \\\n",
       "0  Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                      lowercasing  \\\n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                 punctremoval  \\\n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid19 sekecil apapun di daerahnya masingmasing   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi   \n",
       "1                                  the 75000000 great american patriots who voted for me america first and make america great again will have a giant voice long into the future   they will not be disrespected or treated unfairly in any way shape or form   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                      token  \\\n",
       "0  [saya, meminta, para, kepala, daerah, mewaspadai, kenaikan, kasus, covid19, sekecil, apapun, di, daerahnya, masingmasing, saya, juga, mengingatkan, seluruh, kepala, daerah, agar, terus, mempercepat, vaksinasi, untuk, melindungi, rakyat, sekaligus, mendorong, pertumbuhan, ekonomi]   \n",
       "1                          [the, 75000000, great, american, patriots, who, voted, for, me, america, first, and, make, america, great, again, will, have, a, giant, voice, long, into, the, future, they, will, not, be, disrespected, or, treated, unfairly, in, any, way, shape, or, form]   \n",
       "\n",
       "                                                                                                                                                                                                                                                       stem  \n",
       "0           [saya, minta, para, kepala, daerah, waspada, naik, kasus, covid19, kecil, apa, di, daerah, masingmasing, saya, juga, ingat, seluruh, kepala, daerah, agar, terus, cepat, vaksinasi, untuk, lindung, rakyat, sekaligus, dorong, tumbuh, ekonomi]  \n",
       "1  [the, 75000000, great, american, patriot, who, vote, for, me, america, first, and, make, america, great, again, will, have, a, giant, voic, long, into, the, futur, they, will, not, be, disrespect, or, treat, unfairli, in, ani, way, shape, or, form]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Stemmer\n",
    "\n",
    "## Trump (menggunakan porter-stemmer)\n",
    "trump_stemmed=[porter_stemmer.stem(word=word) for word in df.token[1]]\n",
    "\n",
    "## Jokowi (menggunakan sastrawi)\n",
    "jokowi_stemmed=[stemmer.stem(kata) for kata in df.token[0]]\n",
    "\n",
    "df = df.assign(\n",
    "  stem = [jokowi_stemmed, trump_stemmed]\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Removal\n",
    "\n",
    "Menghilangkan kata-kata yang dinilai tidak memberi nilai tambah dalam proses analisis.\n",
    "\n",
    "Stopwords dapat berbeda-beda tergantung masalah yang dihadapi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:50:05.130633Z",
     "start_time": "2021-10-29T02:50:05.101892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lowercasing</th>\n",
       "      <th>punctremoval</th>\n",
       "      <th>token</th>\n",
       "      <th>stem</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jokowi</td>\n",
       "      <td>Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.</td>\n",
       "      <td>saya meminta para kepala daerah mewaspadai kenaikan kasus covid19 sekecil apapun di daerahnya masingmasing   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi</td>\n",
       "      <td>[saya, meminta, para, kepala, daerah, mewaspadai, kenaikan, kasus, covid19, sekecil, apapun, di, daerahnya, masingmasing, saya, juga, mengingatkan, seluruh, kepala, daerah, agar, terus, mempercepat, vaksinasi, untuk, melindungi, rakyat, sekaligus, mendorong, pertumbuhan, ekonomi]</td>\n",
       "      <td>[saya, minta, para, kepala, daerah, waspada, naik, kasus, covid19, kecil, apa, di, daerah, masingmasing, saya, juga, ingat, seluruh, kepala, daerah, agar, terus, cepat, vaksinasi, untuk, lindung, rakyat, sekaligus, dorong, tumbuh, ekonomi]</td>\n",
       "      <td>[kepala, daerah, waspada, covid19, daerah, masingmasing, kepala, daerah, cepat, vaksinasi, lindung, rakyat, dorong, tumbuh, ekonomi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump</td>\n",
       "      <td>The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!</td>\n",
       "      <td>the 75000000 great american patriots who voted for me america first and make america great again will have a giant voice long into the future   they will not be disrespected or treated unfairly in any way shape or form</td>\n",
       "      <td>[the, 75000000, great, american, patriots, who, voted, for, me, america, first, and, make, america, great, again, will, have, a, giant, voice, long, into, the, future, they, will, not, be, disrespected, or, treated, unfairly, in, any, way, shape, or, form]</td>\n",
       "      <td>[the, 75000000, great, american, patriot, who, vote, for, me, america, first, and, make, america, great, again, will, have, a, giant, voic, long, into, the, futur, they, will, not, be, disrespect, or, treat, unfairli, in, ani, way, shape, or, form]</td>\n",
       "      <td>[75000000, great, american, patriot, vote, america, first, make, america, great, giant, voic, long, futur, disrespect, treat, unfairli, ani, way, shape, form]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  \\\n",
       "0  Jokowi   \n",
       "1   Trump   \n",
       "\n",
       "                                                                                                                                                                                                                                                            tweet  \\\n",
       "0  Saya meminta para kepala daerah mewaspadai kenaikan kasus Covid-19 sekecil apapun di daerahnya masing-masing.   Saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future.   They will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                      lowercasing  \\\n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid-19 sekecil apapun di daerahnya masing-masing.   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi.   \n",
       "1                            the 75,000,000 great american patriots who voted for me, america first, and make america great again, will have a giant voice long into the future.   they will not be disrespected or treated unfairly in any way, shape or form!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                 punctremoval  \\\n",
       "0  saya meminta para kepala daerah mewaspadai kenaikan kasus covid19 sekecil apapun di daerahnya masingmasing   saya juga mengingatkan seluruh kepala daerah agar terus mempercepat vaksinasi untuk melindungi rakyat sekaligus mendorong pertumbuhan ekonomi   \n",
       "1                                  the 75000000 great american patriots who voted for me america first and make america great again will have a giant voice long into the future   they will not be disrespected or treated unfairly in any way shape or form   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                      token  \\\n",
       "0  [saya, meminta, para, kepala, daerah, mewaspadai, kenaikan, kasus, covid19, sekecil, apapun, di, daerahnya, masingmasing, saya, juga, mengingatkan, seluruh, kepala, daerah, agar, terus, mempercepat, vaksinasi, untuk, melindungi, rakyat, sekaligus, mendorong, pertumbuhan, ekonomi]   \n",
       "1                          [the, 75000000, great, american, patriots, who, voted, for, me, america, first, and, make, america, great, again, will, have, a, giant, voice, long, into, the, future, they, will, not, be, disrespected, or, treated, unfairly, in, any, way, shape, or, form]   \n",
       "\n",
       "                                                                                                                                                                                                                                                       stem  \\\n",
       "0           [saya, minta, para, kepala, daerah, waspada, naik, kasus, covid19, kecil, apa, di, daerah, masingmasing, saya, juga, ingat, seluruh, kepala, daerah, agar, terus, cepat, vaksinasi, untuk, lindung, rakyat, sekaligus, dorong, tumbuh, ekonomi]   \n",
       "1  [the, 75000000, great, american, patriot, who, vote, for, me, america, first, and, make, america, great, again, will, have, a, giant, voic, long, into, the, futur, they, will, not, be, disrespect, or, treat, unfairli, in, ani, way, shape, or, form]   \n",
       "\n",
       "                                                                                                                                                            clean  \n",
       "0                            [kepala, daerah, waspada, covid19, daerah, masingmasing, kepala, daerah, cepat, vaksinasi, lindung, rakyat, dorong, tumbuh, ekonomi]  \n",
       "1  [75000000, great, american, patriot, vote, america, first, make, america, great, giant, voic, long, futur, disrespect, treat, unfairli, ani, way, shape, form]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Stop Word Remover\n",
    "\n",
    "## Jokowi (dengan Stopword Bahasa Indonesia)\n",
    "jokowi_clean = [i for i in df.stem[0] if i not in stop_id]\n",
    "\n",
    "## Trump (dengan Stopword Bahasa Inggris)\n",
    "trump_clean = [i for i in df.stem[1] if i not in stop_en]\n",
    "\n",
    "df = df.assign(\n",
    "  clean=[jokowi_clean, trump_clean]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Data Preprocessing\n",
    "- Parsing : menghilangkan tag/komponen teks yang mengganggu (misal tag HTML).\n",
    "- Part-of-Speech Tagging : Mendeteksi posisi kata dalam kalimat (verb, noun, adverb, etc).\n",
    "- Name Entity Recognizion : Mendeteksi nama entitas (orang, tempat, dsb) dalam suatu teks.\n",
    "- Coreference Resolution : Menambahkan referensi yang terhubung atas suatu teks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:51:47.864979Z",
     "start_time": "2021-10-29T02:51:47.752072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('75000000', 'CD'),\n",
       " ('great', 'JJ'),\n",
       " ('american', 'JJ'),\n",
       " ('patriots', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('voted', 'VBD'),\n",
       " ('for', 'IN'),\n",
       " ('me', 'PRP'),\n",
       " ('america', 'VBP'),\n",
       " ('first', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('make', 'VB'),\n",
       " ('america', 'JJ'),\n",
       " ('great', 'JJ'),\n",
       " ('again', 'RB'),\n",
       " ('will', 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('giant', 'JJ'),\n",
       " ('voice', 'NN'),\n",
       " ('long', 'RB'),\n",
       " ('into', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('future', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('disrespected', 'VBN'),\n",
       " ('or', 'CC'),\n",
       " ('treated', 'VBN'),\n",
       " ('unfairly', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('any', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('shape', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('form', 'NN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Contoh : POS-Tagging (NLTK sementara baru menyediakan untuk bahasa Inggris)\n",
    "## Sebagai catatan POS-Tagging direkomendasikan sebelum lemmatization.\n",
    "\n",
    "trump_pos = nltk.tag.pos_tag(df.token[1])\n",
    "\n",
    "list(trump_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector : Sekumpulan/himpunan angka/numeric\n",
    "\n",
    "__Mengapa harus vector?__\n",
    "Komputer tidak dapat memproses arti dan struktur yang ada dalam suatu teks karena bentuk masih berupa non-numeric. Untuk itu, perlu memproses arti dan struktur dari teks tersebut agar dapat menjadi bentuk angka atau kumpulan angka. \n",
    "\n",
    "Hasil proses mengkonversi teks menjadi angka tersebut akan menjadi feature-feature yang dapat digunakan untuk analisis seperti prediksi sentimen dan lain-lain.\n",
    "\n",
    "Di sisi lain, Processor (CPU/GPU) modern didesain untuk melakukan pemrosesan/penghitungan vector.\n",
    "\n",
    "Ada banyak teknik feature generation dari teks, namun akan di bahas secara detail salah satu nya yaitu VSM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector space model\n",
    "\n",
    "Adalah vector yang merepresentasikan tiap dokumen (kalimat) dalam sebuah corpus (kumpulan dokumen) sebagai vektor (list/array) di mana setiap element pada vektor tersebut merupakan kemunculan (occurrence) tiap kata/token di dalam dokumen, dimana nilai kemunculan dari tiap kata tersebut dapat pula diberikan pembobotan.\n",
    "\n",
    "$$ V_{(c)} = \\begin{bmatrix}\n",
    " w_{1,1}&w_{2,1}&...&w_{T,1} \\\\ \n",
    " w_{1,2}&w_{2,2}&...&w_{T,2} \\\\ \n",
    " \\vdots&\\vdots&&\\vdots \\\\\n",
    " w_{1,D}&w_{2,D}&...&w_{T,D}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "- c = corpus\n",
    "- D = jumlah dokumen/kalimat dalam corpus\n",
    "- T = jumlah vocabulary/term/token yang ada dalam seluruh corpus\n",
    "- $w_{1,1}$ = bobot kata pertama pada teks pertama\n",
    "\n",
    "Pertimbangan menggunakan VSM:\n",
    "- VSM memiliki asumsi bahwa urutan token di dalam teks tidak terlalu berarti, namun kemunculan tokens lah yang menunjukan arti dan pengaruh token tersebut.\n",
    "- Asumsi tersebut tidak jadi masalah untuk beberapa jenis analisis teks seperti document classification atau clustering. Kumpulan angka dari tiap kata biasanya cukup untuk membedakan semantic concept dari sebuah dokumen.\n",
    "- Ada beberapa jenis analisis teks yang memerlukan informasi berupa urutan kata dalam kalimat seperti information extraction, POS Tagging dan lain-lain\n",
    "- VSM akan menghasilkan matrik yang memiliki dimensi sepanjang jumlah kata dalam corpus. Butuh resources untuk menganalisis matrik tersebut.\n",
    "\n",
    "Cara pembobotan yang umum dilakukan dalam VSM:\n",
    "- binary values\n",
    "- bag of word (frequency count)\n",
    "- TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:58:42.153688Z",
     "start_time": "2021-10-29T02:58:42.131537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analisis',\n",
       " 'cleansing',\n",
       " 'dan',\n",
       " 'data',\n",
       " 'dengan',\n",
       " 'itu',\n",
       " 'melingkupi',\n",
       " 'penting',\n",
       " 'sama',\n",
       " 'science']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengaplikasikan Vektor Space Secara Manual\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pendapat = {\n",
    "  'pendapat1': 'Data analisis itu penting',\n",
    "  'pendapat2': 'Data cleansing itu sama penting dengan data analisis',\n",
    "  'Pendapat3': 'Data science melingkupi data analisis dan data cleansing'\n",
    "}\n",
    "\n",
    "pendapat_splitted = {\n",
    "  k: v.lower().split() for k, v in pendapat.items() \n",
    "}\n",
    "\n",
    "gabungan = \" \".join([value for key, value in pendapat.items()])\n",
    "\n",
    "vektor = sorted(list(set(gabungan.lower().split())))\n",
    "\n",
    "vektor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:59:30.085262Z",
     "start_time": "2021-10-29T02:59:30.070834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analisis</th>\n",
       "      <th>cleansing</th>\n",
       "      <th>dan</th>\n",
       "      <th>data</th>\n",
       "      <th>dengan</th>\n",
       "      <th>itu</th>\n",
       "      <th>melingkupi</th>\n",
       "      <th>penting</th>\n",
       "      <th>sama</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pendapat1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pendapat2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pendapat3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           analisis  cleansing  dan  data  dengan  itu  melingkupi  penting  \\\n",
       "pendapat1         1          0    0     1       0    1           0        1   \n",
       "pendapat2         1          1    0     1       1    1           0        1   \n",
       "Pendapat3         1          1    1     1       0    0           1        0   \n",
       "\n",
       "           sama  science  \n",
       "pendapat1     0        0  \n",
       "pendapat2     1        0  \n",
       "Pendapat3     0        1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### Binary ######################\n",
    "\n",
    "binary_dict = dict()\n",
    "\n",
    "def binary_count(keyword, lst):\n",
    "  # mengembalikan nilai 1 jika keyword tertentu ada di dalam list kata \n",
    "  return 1 if keyword in lst else 0 \n",
    "\n",
    "for item in vektor:\n",
    "  lst = [binary_count(item, v) for k, v in pendapat_splitted.items()]\n",
    "  binary_dict[item] = lst\n",
    "\n",
    "binary = pd.DataFrame(binary_dict, index=list(pendapat.keys()))\n",
    "binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:00:04.420678Z",
     "start_time": "2021-10-29T03:00:04.393837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analisis</th>\n",
       "      <th>cleansing</th>\n",
       "      <th>dan</th>\n",
       "      <th>data</th>\n",
       "      <th>dengan</th>\n",
       "      <th>itu</th>\n",
       "      <th>melingkupi</th>\n",
       "      <th>penting</th>\n",
       "      <th>sama</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pendapat1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pendapat2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pendapat3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           analisis  cleansing  dan  data  dengan  itu  melingkupi  penting  \\\n",
       "pendapat1         1          0    0     1       0    1           0        1   \n",
       "pendapat2         1          1    0     2       1    1           0        1   \n",
       "Pendapat3         1          1    1     3       0    0           1        0   \n",
       "\n",
       "           sama  science  \n",
       "pendapat1     0        0  \n",
       "pendapat2     1        0  \n",
       "Pendapat3     0        1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### Bag of Words ###################### \n",
    "\n",
    "def freq_count(keyword, lst):\n",
    "  # menghitung jumlah \n",
    "  return lst.count(keyword)\n",
    "\n",
    "bagofwords_dict = dict()\n",
    "\n",
    "for item in vektor:\n",
    "  lst = [freq_count(item, v) for k, v in pendapat_splitted.items()]\n",
    "  bagofwords_dict[item] = lst\n",
    "\n",
    "bagofwords = pd.DataFrame(bagofwords_dict, index=list(pendapat.keys()))\n",
    "bagofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:00:38.511678Z",
     "start_time": "2021-10-29T03:00:38.496159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Intermezzo : Menggunakan Scikit Learn untuk membuat vector space\n",
    "\n",
    "#import fungsi countvectorizer dari sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#membuat corpus\n",
    "corpus = [v for k, v in pendapat.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:00:41.242699Z",
     "start_time": "2021-10-29T03:00:41.221996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analisis</th>\n",
       "      <th>cleansing</th>\n",
       "      <th>dan</th>\n",
       "      <th>data</th>\n",
       "      <th>dengan</th>\n",
       "      <th>itu</th>\n",
       "      <th>melingkupi</th>\n",
       "      <th>penting</th>\n",
       "      <th>sama</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pendapat1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pendapat2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pendapat3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           analisis  cleansing  dan  data  dengan  itu  melingkupi  penting  \\\n",
       "pendapat1         1          0    0     1       0    1           0        1   \n",
       "pendapat2         1          1    0     1       1    1           0        1   \n",
       "Pendapat3         1          1    1     1       0    0           1        0   \n",
       "\n",
       "           sama  science  \n",
       "pendapat1     0        0  \n",
       "pendapat2     1        0  \n",
       "Pendapat3     0        1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### Binary ######################\n",
    "# buat vectorizernya sebagai object, pastikan set binary = True\n",
    "countVec = CountVectorizer(binary=True)\n",
    "\n",
    "# train vectorizernya dengan corpus (pandas series/list/numpy array)\n",
    "countVec.fit(corpus)\n",
    "\n",
    "# apply vectorizernya ke corpus (data yang sama) dan simpan hasilnya sebagai object\n",
    "bin_result = countVec.transform(corpus)\n",
    "\n",
    "# convert hasil menjadi pandas dataframe\n",
    "# hasil harus di convert menjadi numpy array karena sebelumnya memiliki type sparse matrix\n",
    "# ubah kolom dengan feature name yang ada pada vectorizer object\n",
    "pd.DataFrame(bin_result.toarray(), columns=countVec.get_feature_names(), index=list(pendapat.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:00:44.719736Z",
     "start_time": "2021-10-29T03:00:44.702493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analisis</th>\n",
       "      <th>cleansing</th>\n",
       "      <th>dan</th>\n",
       "      <th>data</th>\n",
       "      <th>dengan</th>\n",
       "      <th>itu</th>\n",
       "      <th>melingkupi</th>\n",
       "      <th>penting</th>\n",
       "      <th>sama</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pendapat1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pendapat2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pendapat3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           analisis  cleansing  dan  data  dengan  itu  melingkupi  penting  \\\n",
       "pendapat1         1          0    0     1       0    1           0        1   \n",
       "pendapat2         1          1    0     2       1    1           0        1   \n",
       "Pendapat3         1          1    1     3       0    0           1        0   \n",
       "\n",
       "           sama  science  \n",
       "pendapat1     0        0  \n",
       "pendapat2     1        0  \n",
       "Pendapat3     0        1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### Bag of Words ######################\n",
    "\n",
    "# buat vectorizernya sebagai object, pastikan set binary = False\n",
    "countVec = CountVectorizer(binary=False)\n",
    "\n",
    "# train vectorizernya dengan corpus (pandas series/list/numpy array)\n",
    "countVec.fit(corpus)\n",
    "\n",
    "# apply vectorizernya ke corpus (data yang sama) dan simpan hasilnya sebagai object\n",
    "bow_result = countVec.transform(corpus)\n",
    "\n",
    "# convert hasil menjadi pandas dataframe\n",
    "# hasil harus di convert menjadi numpy array karena sebelumnya memiliki type sparse matrix\n",
    "# ubah kolom dengan feature name yang ada pada vectorizer object\n",
    "pd.DataFrame(bow_result.toarray(), columns=countVec.get_feature_names(), index=list(pendapat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency (TF)\n",
    "\n",
    "TF = jumlah kemunculan atas satu kata dalam satu dokumen\n",
    "\n",
    "Kenapa digunakan untuk merepresentasikan sebuah dokumen?\n",
    "- kata-kata yang sering muncul (selain stopword) itu memiliki tema (thematic)\n",
    "- jika sebuah kata sering muncul pada sebuah dokumen bertema A, dokumen lain yang mengandung kata tersebut seharusnya memiliki tema A mirip dengan dokumen sebelumnya\n",
    "\n",
    "Cara-cara menghitung TF:\n",
    "- binary: 0/1\n",
    "- raw frequency: $n_{t,d}$\n",
    "- weighted frequency: $\\frac{n_{t,d}}{N_d}$\n",
    "- sublinear TF: $1 + log(n_{t,d}), 0$ if $n_{t,d} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inversed Document Frequency (IDF)\n",
    "\n",
    "Masalah jika hanya menggunakan TF:\n",
    "- semua kata/token dianggap sama-sama penting\n",
    "- beberapa kata kadang tidak memiliki atau hanya sedikit kemampuan untuk membedakan antar dokumen.\n",
    "\n",
    "Misalnya, pada kumpulan tweet dari tenaga medis untuk kasus sentiment analisis terkait vaksinasi, kata \"pasien\" atau \"penyakit\" mungkin sering muncul di banyak tweet tapi hanya sedikit berkontribusi untuk  membedakan mana tweet yang bersentimen negatif atau positif.\n",
    "\n",
    "Untuk mengurangi masalah ini, perlu adanya mekanisme untuk mengurangi bobot kata/token yang terlalu sering muncul dalam suatu dokumen agar menjadi lebih bermakna bagi penentuan relevansi. Idenya adalah untuk mengurangi bobot TF tiap kata/token dengan seiring dengan banyaknya dokumen yang memiliki kata tersebut. Faktor pengurang tersebut dihitung dengan menggunakan rumus IDF sebagai berikut:\n",
    "\n",
    "$$idf_t = log \\left ( \\frac{D}{n_t}\\right) $$\n",
    "\n",
    "- D = jumlah dokumen dalam corpus\n",
    "- n_t = jumlah dokumen dimana terdapat term t\n",
    "\n",
    "Cara-cara menghitung IDF:\n",
    "- IDF = $log \\left ( \\frac{D}{n_t}\\right)$\n",
    "- smoothed IDF = $log \\left ( \\frac{D}{n_t+1}\\right)$\n",
    "- Sklearn IDF = $log \\left ( \\frac{D+1}{n_t+1}\\right)+1$\n",
    "- Sklearn IDF, smooth_idf off: $log \\left ( \\frac{D}{n_t}\\right)+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "gabungan antara TF dan IDF dari sebuah term/kata\n",
    "\n",
    "$$ TF-IDF = n_{t,d} \\times log \\left ( \\frac{D}{n_t}\\right) $$\n",
    "\n",
    "semakin banyak kata itu muncul di sebuat dokumen/kalimat, semakin relevan kata tersebut untuk membedakan antar dokumen.\n",
    "namun, semakin banyak kata itu juga muncul di banyak dokumen lain, semakin tidak relevan kata tersebut \n",
    "\n",
    "jika suatu kata di temukan dalam seluruh dokumen makan besar tf-idf nya adalah 0, karena idf nya 0($log\\left ( \\frac{D}{D}\\right )$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:05:28.601722Z",
     "start_time": "2021-10-29T03:05:28.567120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analisis</th>\n",
       "      <th>cleansing</th>\n",
       "      <th>dan</th>\n",
       "      <th>data</th>\n",
       "      <th>dengan</th>\n",
       "      <th>itu</th>\n",
       "      <th>melingkupi</th>\n",
       "      <th>penting</th>\n",
       "      <th>sama</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pendapat1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pendapat2</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pendapat3</th>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           analisis  cleansing       dan      data    dengan       itu  \\\n",
       "pendapat1  0.500000   0.000000  0.000000  0.500000  0.000000  0.500000   \n",
       "pendapat2  0.316228   0.316228  0.000000  0.632456  0.316228  0.316228   \n",
       "Pendapat3  0.267261   0.267261  0.267261  0.801784  0.000000  0.000000   \n",
       "\n",
       "           melingkupi   penting      sama   science  \n",
       "pendapat1    0.000000  0.500000  0.000000  0.000000  \n",
       "pendapat2    0.000000  0.316228  0.316228  0.000000  \n",
       "Pendapat3    0.267261  0.000000  0.000000  0.267261  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### TF ######################\n",
    "#import fungsi tfidfvectorizer dari sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# buat vectorizernya sebagai object, pastikan use_idf=False\n",
    "tfidfVec = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "# train vectorizernya dengan corpus (pandas series/list/numpy array)\n",
    "tfidfVec.fit(corpus)\n",
    "\n",
    "# apply vectorizernya ke corpus (data yang sama) dan simpan hasilnya sebagai object\n",
    "tfidf_result = tfidfVec.transform(corpus)\n",
    "\n",
    "# convert hasil menjadi pandas dataframe\n",
    "# hasil harus di convert menjadi numpy array karena sebelumnya memiliki type sparse matrix\n",
    "# ubah kolom dengan feature name yang ada pada vectorizer object\n",
    "pd.DataFrame(tfidf_result.toarray(), columns = tfidfVec.get_feature_names(), index=list(pendapat.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:05:31.399025Z",
     "start_time": "2021-10-29T03:05:31.372211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analisis</th>\n",
       "      <th>cleansing</th>\n",
       "      <th>dan</th>\n",
       "      <th>data</th>\n",
       "      <th>dengan</th>\n",
       "      <th>itu</th>\n",
       "      <th>melingkupi</th>\n",
       "      <th>penting</th>\n",
       "      <th>sama</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pendapat1</th>\n",
       "      <td>0.433708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pendapat2</th>\n",
       "      <td>0.252314</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.504628</td>\n",
       "      <td>0.427205</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.427205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pendapat3</th>\n",
       "      <td>0.222176</td>\n",
       "      <td>0.286092</td>\n",
       "      <td>0.376177</td>\n",
       "      <td>0.666529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           analisis  cleansing       dan      data    dengan       itu  \\\n",
       "pendapat1  0.433708   0.000000  0.000000  0.433708  0.000000  0.558478   \n",
       "pendapat2  0.252314   0.324900  0.000000  0.504628  0.427205  0.324900   \n",
       "Pendapat3  0.222176   0.286092  0.376177  0.666529  0.000000  0.000000   \n",
       "\n",
       "           melingkupi   penting      sama   science  \n",
       "pendapat1    0.000000  0.558478  0.000000  0.000000  \n",
       "pendapat2    0.000000  0.324900  0.427205  0.000000  \n",
       "Pendapat3    0.376177  0.000000  0.000000  0.376177  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### TF-IDF ######################\n",
    "#import fungsi tfidfvectorizer dari sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# buat vectorizernya sebagai object, use_idf secara default bernilai True\n",
    "tfidfVec = TfidfVectorizer()\n",
    "\n",
    "# train vectorizernya dengan corpus (pandas series/list/numpy array)\n",
    "tfidfVec.fit(corpus)\n",
    "\n",
    "# apply vectorizernya ke corpus (data yang sama) dan simpan hasilnya sebagai object\n",
    "tfidf_result = tfidfVec.transform(corpus)\n",
    "\n",
    "# convert hasil menjadi pandas dataframe\n",
    "# simpan ke dalam variabel untuk pemrosesan selanjutnya\n",
    "# hasil harus di convert menjadi numpy array karena sebelumnya memiliki type sparse matrix\n",
    "# ubah kolom dengan feature name yang ada pada vectorizer object\n",
    "tfidf_vector = pd.DataFrame(tfidf_result.toarray(), columns = tfidfVec.get_feature_names(), index=list(pendapat.keys()))\n",
    "tfidf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materi tambahan: Cosine Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In a nutshell__\n",
    "- Merupakan teknik yang digunakan untuk mengukur kesamaan antara dua vektor berdasarkan _inner product_ nya. \n",
    "- Menggunaan cosinus sudut antara dua vektor dan menentukan apakah dua vektor menunjuk ke arah yang kira-kira sama. \n",
    "- Sering digunakan untuk mengukur kesamaan dokumen dalam analisis teks karena tidak terpengaruh oleh ‚Äòcurse of dimensionality‚Äô sehingga perhitungannya lebih cepat.\n",
    "\n",
    "\\\n",
    "\\\n",
    "__Mengukur kemiripan dokumen__\n",
    "\n",
    "terdapat dua teknik umum dalam mengukur kesamaan dokumen:\n",
    "- Jarak antara dua vector dokumen (euclidian distance)\n",
    "- Sudut antara dua vector dokumen \n",
    "\n",
    "<details>\n",
    "    <img src=\"img/image-cosine.png\">\n",
    "</details>\n",
    "\n",
    "\\\n",
    "\\\n",
    "__Mengapa Cosine?__\n",
    "\n",
    "Mari sedikit mengingat trigonometri.\n",
    "\n",
    "| |0¬∞|30¬∞|45¬∞|60¬∞|90¬∞|\n",
    "|:--|:-:|:-:|:-:|:-:|:-:|\n",
    "|sin|0|$$\\frac{1}{2}$$|$$\\frac{\\sqrt{2}}{2}$$|$$\\frac{\\sqrt{3}}{2}$$|1|\n",
    "|cos|1|$$\\frac{\\sqrt{3}}{2}$$|$$\\frac{\\sqrt{2}}{2}$$|$$\\frac{1}{2}$$|0|\n",
    "|tan|0|$$\\frac{\\sqrt{3}}{3}$$|1|$$\\sqrt{3}$$|E|\n",
    "\n",
    "mana yang lebih baik digunakan?\n",
    "\n",
    "\\\n",
    "\\\n",
    "__Menghitung Cosine Similarity__\n",
    "Rumus cosine similarity adalah sbb:\n",
    "\n",
    "$$cos\\theta = \\frac{A.B}{||A||x||B||} $$\n",
    "\n",
    "\\\n",
    "\\\n",
    "__‚ö†Ô∏èHal-hal yang harus diperhatikan pada dalam penggunaan cosine similarity:__\n",
    "- Angka cosine similarity antara 0-1.\n",
    "- Semakin besar angka cosine similarity, semakin tinggi tingkat kemiripan antara dua dokumen.\n",
    "- Angka cosine similarity bukan persentase kemiripan (0.667 bukan berarti dokumen A 67% mirip dokumen B atau cosine similarity 1 bukan berarti dua dokumen sama persis), tapi ukuran sudut kemiripan dua vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:17:01.638050Z",
     "start_time": "2021-10-29T03:17:01.618180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.69119083, 0.38543847],\n",
       "       [0.69119083, 1.        , 0.48535889],\n",
       "       [0.38543847, 0.48535889, 1.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### Menghitung Cosine Similarity ######################\n",
    "######################   Menggunakan Scikit Learn   ######################\n",
    "import numpy as np; import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "## kita gunakan vector hasil pengolahan tfidf\n",
    "similarities = cosine_similarity(tfidf_vector)\n",
    "similarities ##akan menghasilkan Array Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:17:46.362218Z",
     "start_time": "2021-10-29T03:17:45.726924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualisasi Cosine\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "ax = sns.heatmap(similarities, cmap=\"YlGnBu\", linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:19:19.380176Z",
     "start_time": "2021-10-29T03:19:19.355672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dokumen 1</th>\n",
       "      <th>dokumen 2</th>\n",
       "      <th>kemiripan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pendapat1</td>\n",
       "      <td>pendapat2</td>\n",
       "      <td>0.691191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pendapat1</td>\n",
       "      <td>Pendapat3</td>\n",
       "      <td>0.385438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pendapat2</td>\n",
       "      <td>Pendapat3</td>\n",
       "      <td>0.485359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dokumen 1  dokumen 2  kemiripan\n",
       "0  pendapat1  pendapat2   0.691191\n",
       "1  pendapat1  Pendapat3   0.385438\n",
       "2  pendapat2  Pendapat3   0.485359"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## menyimpan nilai cosine ke dalam dataframe pandas\n",
    "\n",
    "doc_list = list(pendapat.keys())\n",
    "\n",
    "doc1 = list()\n",
    "doc2 = list()\n",
    "cosine_value = list()\n",
    "\n",
    "for i in range(tfidf_vector.shape[0]-1):\n",
    "  for j in range(i+1, tfidf_vector.shape[0]):\n",
    "    doc1.append(doc_list[i])\n",
    "    doc2.append(doc_list[j])\n",
    "    cosine_value.append(similarities[i][j])\n",
    "\n",
    "cosine_dict = {\n",
    "  'dokumen 1': doc1,\n",
    "  'dokumen 2': doc2,\n",
    "  'kemiripan': cosine_value \n",
    "}\n",
    "\n",
    "pd.DataFrame(cosine_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh Modelling untuk Case NLP - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset sentiment dari Scraping Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:14:33.662190Z",
     "start_time": "2021-10-28T13:14:33.632794Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/datalatihannlp.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Sederhana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:14:33.677767Z",
     "start_time": "2021-10-28T13:14:33.664199Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:14:33.693651Z",
     "start_time": "2021-10-28T13:14:33.679250Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:14:33.819424Z",
     "start_time": "2021-10-28T13:14:33.695290Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x=data['Label'].value_counts().index, y=data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kita perlu mengubah data pada kolom ```Label``` menjadi binary karena masih berupa text sesuai dengan kelompoknya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:14:33.834657Z",
     "start_time": "2021-10-28T13:14:33.820423Z"
    }
   },
   "outputs": [],
   "source": [
    "data['target'] = data['Label'].map({'negatif':0,'positif':1})\n",
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proses Cleansing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:14:33.850659Z",
     "start_time": "2021-10-28T13:14:33.836505Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = list(stopwords.words('indonesian'))\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:14:33.866110Z",
     "start_time": "2021-10-28T13:14:33.851659Z"
    }
   },
   "outputs": [],
   "source": [
    "stop.remove('tidak')\n",
    "print(len(stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kita dapat melihat bahwa di setiap tweet terdapat string 'b' yang tidak kita butuhkan, untuk itu kita akan ambil tweet setelah string 'b' tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:14:33.881321Z",
     "start_time": "2021-10-28T13:14:33.868000Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_text = data['text']\n",
    "train = [item[1:] for item in tweet_text]\n",
    "train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setelah itu kita akan melakukan proses cleansing dari semua yang sudah kita pelajari di atas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:18:37.897884Z",
     "start_time": "2021-10-28T13:14:33.883489Z"
    }
   },
   "outputs": [],
   "source": [
    "#Proses Pembersihan Data\n",
    "#Import Library\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import re\n",
    "#1. Mengecilkan Huruf\n",
    "hasil = [item.lower() for item in train]\n",
    "#2. Menghilangkan Angka, Alamat HTTP, Tanda Baca, Angka, dan Whitespace\n",
    "hasil = [' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", item).split()) for item in hasil]\n",
    "hasil = [' '.join(re.sub(r\"\\d+\", \"\", item).split()) for item in hasil]\n",
    "#3.Stopwords\n",
    "stopwords = set(stopwords.words('indonesian'))\n",
    "tampung = []\n",
    "for item in hasil:\n",
    "  querywords = item.split()\n",
    "  resultwords  = [word for word in querywords if word not in stopwords]\n",
    "  result = ' '.join(resultwords)\n",
    "  tampung.append(result)\n",
    "\n",
    "#4. Stemming\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "hasil = [stemmer.stem(item) for item in tampung]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kita akan membuat dataframe baru yang berisi tweet yang sudah dibersihkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:18:37.928633Z",
     "start_time": "2021-10-28T13:18:37.899939Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_tweet = hasil\n",
    "sentiment = data['Label']\n",
    "df = pd.DataFrame({'Text':clean_tweet,'Label':sentiment})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persiapan Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sebelum melakukan training, kita akan melakukan proses Train-Test Split untuk membagi dataset kita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:18:37.944045Z",
     "start_time": "2021-10-28T13:18:37.929693Z"
    }
   },
   "outputs": [],
   "source": [
    "#membagi dataset ke dalam train dan test dengan menggunakan libray train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#perhatikan parameter yang digunakan saat membagi dataset\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Text'],\n",
    "                                                    df['Label'],\n",
    "                                                    shuffle = True,\n",
    "                                                    stratify=df['Label'],\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 42\n",
    "                                                   )\n",
    "\n",
    "print('Baris Training Set \\t:', str(len(x_train)))\n",
    "print('Baris Test Set \\t\\t:', str(len(x_test)))\n",
    "\n",
    "#distribusi dari masing-masing training set dan testing set atas variable y yang dalam hal ini adalah sentimen negatif dan positif\n",
    "\n",
    "print('\\nDistribusi target variable training set:\\n', y_train.value_counts())\n",
    "print('\\nDistribusi target variable test set:\\n', y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:18:37.989840Z",
     "start_time": "2021-10-28T13:18:37.945008Z"
    }
   },
   "outputs": [],
   "source": [
    "#mengimplementasikan TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#memanggil library\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#melakukan fitting ke train\n",
    "\n",
    "vectorizer.fit(x_train)\n",
    "\n",
    "#melakukan transformasi atas x_train ke dalam bentuk vektor\n",
    "\n",
    "x_train_vec = vectorizer.transform(x_train)\n",
    "\n",
    "#jumlah keseluruhan fitur yang berhasil di ekstraksi atas implementasi TF-IDF\n",
    "\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:18:38.005399Z",
     "start_time": "2021-10-28T13:18:37.991773Z"
    }
   },
   "outputs": [],
   "source": [
    "#print skor tf-idf dari tokens\n",
    "\n",
    "tfidf_result = pd.DataFrame(x_train_vec[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "tfidf_result = tfidf_result.sort_values('TF-IDF', ascending=False)\n",
    "print (tfidf_result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library untuk Modelling (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:18:38.021521Z",
     "start_time": "2021-10-28T13:18:38.006396Z"
    }
   },
   "outputs": [],
   "source": [
    "#menggunakan algoritma ComplementNB untuk melakukan prediksi\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "#memanggil library\n",
    "\n",
    "model = ComplementNB(0.01)\n",
    "\n",
    "#melakukan fitting atas training set, dalam hal ini x_train_vec (xtrain dalam bentuk vektor) dan y_train\n",
    "\n",
    "model.fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya kita akan mengukur kemampuan dari model dalam melakukan prediksi menggunakan accuracy_score dan confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:31:24.238690Z",
     "start_time": "2021-10-28T13:31:24.208008Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay,classification_report\n",
    "\n",
    "#melakukan transformasi atas x_test ke dalam bentuk vektor\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "#melakukan prediksi atas x_test_vec (x_test dalam bentuk vektor)\n",
    "prediction = model.predict(x_test_vec)\n",
    "\n",
    "#melihat accuracy_score\n",
    "print('Score akurasi model:',str(accuracy_score(y_test,prediction)))\n",
    "\n",
    "#melihat confusion_score\n",
    "cm_test = confusion_matrix(y_test, prediction, labels=model.classes_)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=model.classes_)\n",
    "disp_test.plot(cmap='Blues').ax_.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:31:26.511441Z",
     "start_time": "2021-10-28T13:31:26.485774Z"
    }
   },
   "outputs": [],
   "source": [
    "#Classification Report\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:31:29.304044Z",
     "start_time": "2021-10-28T13:31:29.293068Z"
    }
   },
   "outputs": [],
   "source": [
    "#print perbandingan prediksi mesin dan jawaban\n",
    "\n",
    "pd.DataFrame({'text':x_test, 'true label':y_test,'prediction':prediction}).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kita juga dapat melakukan ```tuning hyperparameter``` untuk mendapatkan akurasi model yang lebih baik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan hyperparameter tuning, artinya mengubah parameter dalam algoritma dalam rangka memperoleh hasil terbaik, dalam hal ini accuracy_score dan confusion_matrix, disini menggunakan library GridSearchCV yang artinya seluruh parameter yang kita masukan ke dalam param_grid, akan dicoba satu-persatu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:31:33.555984Z",
     "start_time": "2021-10-28T13:31:32.107362Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#perhatikan setting dari masing-masing parameter di GridSearchCV\n",
    "parameters = { \n",
    "    'alpha' : [0.01, 0.1, 1]\n",
    "     }\n",
    "grid_search_nb = GridSearchCV(model, parameters, n_jobs=4, cv =10,verbose = 0, return_train_score=True).fit(x_train_vec, y_train)\n",
    "print(grid_search_nb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:31:36.151228Z",
     "start_time": "2021-10-28T13:31:36.123855Z"
    }
   },
   "outputs": [],
   "source": [
    "#menggunakan parameter terbaik ke dalam model\n",
    "best_model = ComplementNB(1)\n",
    "best_model.fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:31:38.722086Z",
     "start_time": "2021-10-28T13:31:38.690718Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "prediction_tuned = best_model.predict(x_test_vec)\n",
    "\n",
    "print('Score akurasi model setelah dituning :',str(accuracy_score(y_test,prediction_tuned)))\n",
    "\n",
    "#melihat confusion_score\n",
    "cm_test = confusion_matrix(y_test, prediction_tuned, labels=model.classes_)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=model.classes_)\n",
    "disp_test.plot(cmap='Blues').ax_.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T13:18:38.585255Z",
     "start_time": "2021-10-28T13:18:38.585255Z"
    }
   },
   "outputs": [],
   "source": [
    "#Classification Report\n",
    "print(classification_report(y_test, prediction_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta$$"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "7c520343219892f1943782ea25bd8e96992f4aa90018e9cd5004f4b06d021ab4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "themify": {
   "theme": "MOF-DAC 1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
